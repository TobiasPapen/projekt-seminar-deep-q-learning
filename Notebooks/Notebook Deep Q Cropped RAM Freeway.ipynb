{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preinstallation of modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gym[atari]\n",
    "#pip install ale_py\n",
    "#pip install autorom[accept-rom-license]\n",
    "#pip install torch\n",
    "#conda install pytorch-cuda = 11.6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipReward(gym.RewardWrapper):\n",
    "    #Method to initialize the updated reward function\n",
    "    def __init__(self, env, min_reward, max_reward):\n",
    "        #Inheritance: calling construtor of the super class\n",
    "        super().__init__(env)\n",
    "\n",
    "        #Saving the given parameters\n",
    "        self.pre_pos = 6                                    #previous position of the chicken (lowest value = 6)\n",
    "        self.min_reward = min_reward                        #minimum reward\n",
    "        self.max_reward = max_reward                        #maximum reward\n",
    "        self.reward_range = (min_reward, max_reward)        #range of the reward\n",
    "\n",
    "        #Initialising temporary parameters\n",
    "        self.targetRuns = 0                                 #number of target runs\n",
    "        self.crashCount=0                                   #number of crashes in a game\n",
    "        self.ladder=[0]*180                                 #unique y positions\n",
    "\n",
    "    def reward(self, reward):\n",
    "        #Recieving the RAM\n",
    "        ram = env.unwrapped.ale.getRAM()\n",
    "        #Saving the current position and the cooldown value of the chicken\n",
    "        #The cooldown value is not zero if the chicken got hit, was reseted and now needs to wait for a short time\n",
    "        current_pos = ram[14]\n",
    "        cooldown = ram[106]\n",
    "        \n",
    "        reward = 0\n",
    "        if(self.pre_pos<current_pos and self.ladder[current_pos]==0):   #is ladder position new?\n",
    "            reward += 50                                                #Reward for visiting unique (before next target run) position \n",
    "            self.ladder[current_pos]=1                                  #ladder position not new anymore\n",
    "        \n",
    "        #Chicken reaches the goal\n",
    "        if 140 <= ram[106] <= 141:\n",
    "            reward += 10000\n",
    "            self.targetRuns+=1\n",
    "            self.self.ladder=[0]*180                                    #reset ladder positions to unique\n",
    "        \n",
    "        reward += (ram[14] * 0.5) - 50                                  #Reward for having current position\n",
    "        #Chicken got run over\n",
    "        if 90 <= ram[106] <= 100:\n",
    "            reward -= 100\n",
    "            self.crashCount+=1\n",
    "        else:\n",
    "            reward += 0\n",
    "        self.pre_pos = current_pos                                      #Current position will be the previous position in the next transition\n",
    "        return np.clip(reward, self.min_reward, self.max_reward)\n",
    "    #get methods\n",
    "    def getTargetRuns(self):\n",
    "        return self.targetRuns                                         \n",
    "    def getCrashCount(self):\n",
    "        return self.crashCount\n",
    "\n",
    "    #reset methods\n",
    "    def targetReset(self):                                              #reset temporary parameters\n",
    "        self.targetRuns = 0\n",
    "        self.crashCount = 0\n",
    "        self.ladder=[0]*180"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Q Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    #Method to initialize the dqn\n",
    "    def __init__(self, lr, dims_input, dims_fc1, dims_fc2, n_actions):\n",
    "        #Inheritance: calling the constructor of the super class\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        #Saving the given parameters\n",
    "        self.lr = lr                                                #Learning rate\n",
    "        self.dims_input = dims_input                                #Dimensions of the input\n",
    "        self.dims_fc1 = dims_fc1                                    #Dimensions of the first fully connected layer\n",
    "        self.dims_fc2 = dims_fc2                                    #Dimensions of the second fully connected layer\n",
    "        self.n_actions = n_actions                                  #Number of possible actions\n",
    "\n",
    "        #On each layer a linear transformation is applied\n",
    "        self.fc1 = nn.Linear(self.dims_input, self.dims_fc1)\n",
    "        self.fc2 = nn.Linear(self.dims_fc1, self.dims_fc2)\n",
    "        self.fc3 = nn.Linear(self.dims_fc2, self.n_actions)\n",
    "\n",
    "        #Setting up the optimization function which is using the adam algorithm\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr = self.lr)\n",
    "        \n",
    "        #Setting up the mean squared error loss function\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "        #Choosing a gpu if possible\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    #Method to calculate actions\n",
    "    def forward(self, state):\n",
    "        #Applying the relu activiation function to the first two layers\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        #No activation function is applied to the outputs\n",
    "        actions = self.fc3(x)\n",
    "\n",
    "        #Returing the values for each action\n",
    "        return actions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    #Method to initialize the agent\n",
    "    def __init__(self, gamma, lr, epsilon, eps_dec, eps_end, dims_input, dims_fc1, dims_fc2, batch_size, max_mem_size, n_actions):\n",
    "        #Saving the given parameters\n",
    "        self.gamma = gamma                                             #Discount factor\n",
    "        self.lr = lr                                                   #Learning rate        \n",
    "        self.epsilon = epsilon                                         #Epsilon start value\n",
    "        self.epsilonTmp = self.epsilon                                 #Epsilon start value for epsilon reset\n",
    "        self.eps_dec = eps_dec                                         #Epsilon descent valze\n",
    "        self.eps_end = eps_end                                         #Epsilon final value\n",
    "        self.dims_input = dims_input                                   #Dimension of the input\n",
    "        self.dims_fc1 = dims_fc1                                       #Dimension of the first fully connected layer\n",
    "        self.dims_fc2 = dims_fc2                                       #Dimension of the second fully connected layer\n",
    "        self.batch_size = batch_size                                   #Batch size\n",
    "        self.max_mem_size = max_mem_size                               #Maximum memory size\n",
    "        self.n_actions = n_actions                                     #Number of possible actions\n",
    "        self.action_space = [i for i in range(n_actions)]              #Action space\n",
    "        self.mem_counter = 0                                           #Counter for the memory\n",
    "        self.runs = 0                                                  #Number of runs\n",
    "\n",
    "        #Initializing our dqn (which is using two fully conected layers)\n",
    "        self.Q_eval = DeepQNetwork(self.lr, self.dims_input, self.dims_fc1, self.dims_fc2, self.n_actions)\n",
    "\n",
    "        #Initializing our memory (state, new state, action, reward & termination)\n",
    "        self.state_memory = np.zeros((self.max_mem_size, self.dims_input), dtype = np.float32)\n",
    "        self.new_state_memory = np.zeros((self.max_mem_size, self.dims_input), dtype = np.float32)\n",
    "        self.action_memory = np.zeros(self.max_mem_size, dtype = np.int32)\n",
    "        self.reward_memory = np.zeros(self.max_mem_size, dtype = np.float32)\n",
    "        self.terminal_memory = np.zeros(self.max_mem_size, dtype = np.bool_)\n",
    "    \n",
    "    #Method to store a transition inside the memory\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        #Determining the storage index\n",
    "        index = self.mem_counter % self.max_mem_size\n",
    "        \n",
    "        #Storing the given state, new state, reward, action and termination information\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.reward_memory[index] = reward\n",
    "        self.action_memory[index] = action\n",
    "        self.terminal_memory[index] = done\n",
    "\n",
    "        #Increasing the memory counter by one\n",
    "        self.mem_counter += 1\n",
    "    \n",
    "    #Method to choose an action\n",
    "    def choose_action(self, observation):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            #Exploitation\n",
    "            state = T.tensor(observation).to(self.Q_eval.device)\n",
    "            actions = self.Q_eval.forward(state)\n",
    "            #Choosing the best action (argmax returns the index of the highest value)\n",
    "            action = T.argmax(actions).item()\n",
    "        else:\n",
    "            #Exploration (choosing a random action)\n",
    "            action = np.random.choice(self.action_space,p=[.1,.8,.1])#Stehen, vor, zurueck\n",
    "\n",
    "        #Returning the selected actions\n",
    "        return action\n",
    "\n",
    "    #Method to let the agent learn\n",
    "    def learn(self):\n",
    "        if self.mem_counter < self.batch_size:\n",
    "            #Since the memory is empty at the beginning, we are filling it before the agent starts to learn \n",
    "            return\n",
    "        \n",
    "        #Setting the gradients of the optimized tensors to zero (otherwise these are cumulated by pytorch)\n",
    "        self.Q_eval.optimizer.zero_grad()\n",
    "\n",
    "        #Getting the highest memory index we can reach\n",
    "        max_mem = min(self.mem_counter, self.max_mem_size)\n",
    "\n",
    "        #Choosing a random memory entry (as many times as a batch is big)\n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace = False)\n",
    "\n",
    "        #Creating an array with the batch_size in order to slice the calculated values for the actions later on\n",
    "        batch_index = np.arange(self.batch_size, dtype = np.int32)\n",
    "\n",
    "        #Getting the states, new states, rewards, actions and termination information for the batch\n",
    "        state_batch = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n",
    "        new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\n",
    "        reward_batch = T.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\n",
    "        action_batch = self.action_memory[batch]                                            #This is not a tensor since we need it for slicing later\n",
    "        terminal_batch = T.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n",
    "\n",
    "        #Calculating the estimate during the previous state\n",
    "        #Additional slicing in order to get the estimate for the action the agent selected \n",
    "        q_eval = self.Q_eval.forward(state_batch)[batch_index, action_batch]                #The estimates for the actions the agent took\n",
    "        #Calculating the estimate for the next state\n",
    "        q_next = self.Q_eval.forward(new_state_batch)\n",
    "        #Setting values of terminal states to zero (there are no next states after the game finished)\n",
    "        q_next[terminal_batch] = 0.0\n",
    "        #Calculating the target values\n",
    "        q_target = reward_batch + self.gamma * T.max(q_next, dim = 1)[0]\n",
    "        #Calculating the loss (via mean squared error)\n",
    "        loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n",
    "        #Perfoming backpropagation\n",
    "        loss.backward()\n",
    "        #Updating (optimizing) the parameters \n",
    "        self.Q_eval.optimizer.step()\n",
    "\n",
    "        #Increasing the number of runs\n",
    "        self.runs += 1\n",
    "\n",
    "    #Method to recieve a model\n",
    "    def getModel(self):\n",
    "        return self.Q_eval\n",
    "\n",
    "    #Method to set a trained model\n",
    "    def setModel(self, path):\n",
    "        self.Q_eval = T.load(path)\n",
    "\n",
    "    #Method to decrease the epsilon value\n",
    "    def decreaseEpsilon(self):\n",
    "        self.epsilon = self.epsilon - self.eps_dec if (self.epsilon - self.eps_dec) > self.eps_end else self.eps_end\n",
    "\n",
    "    #Method to increase the discount factor gamma\n",
    "    def increaseGamma(self, increment):\n",
    "        self.gamma = self.gamma + increment\n",
    "\n",
    "    #Method to reset Epsilon\n",
    "    def resetEps(self):\n",
    "        self.epsilon=self.epsilonTmp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epsilon Decrease Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019799999999999998\n"
     ]
    }
   ],
   "source": [
    "gue=50                              #games until final epsilon value\n",
    "efv=0.01                            #Epsilon final value, Set 0.0 for testing of trained Model\n",
    "ep_dec_calc=(1.0-efv)/gue           #calculated epsilon decrease per game\n",
    "print(ep_dec_calc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modell Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting all relevant parameters for the dqn:\n",
    "p_n_games = 400                     #Number of games\n",
    "p_n_games_save=50                   #frequency of saved model\n",
    "gue=p_n_games*0.75                  #games until final epsilon value\n",
    "p_epsilon = 1.0                     #Epsilon start value, Set 0.0 for testing of trained Model\n",
    "p_epsilon_reset=True                #reset epsilon after p_n_games_save games\n",
    "p_eps_end = .01                     #Epsilon final value, Set 0.0 for testing of trained Model\n",
    "p_eps_dec = (1.0-p_eps_end)/gue     #Epsilon descent value\n",
    "if(p_epsilon_reset):                #Epsilon descent value if epsilon is reset every p_n_games_save games\n",
    "    p_eps_dec = (1.0-p_eps_end)/(p_n_games_save*0.75)\n",
    "p_lr = 0.005                        #Learning rate\n",
    "p_gamma = 0.99                      #Discount factor \n",
    "p_dims_input = 13                   #Dimension of the input\n",
    "p_dims_fc1 = 1024                   #Dimension of the first fully connected layer\n",
    "p_dims_fc2 = 1024                   #Dimension of the second fully connected layer\n",
    "p_batch_size = 64                   #Batch size\n",
    "p_max_mem_size = 1000000            #Maximum memory size\n",
    "p_n_actions = 3                     #Number of possible actions\n",
    "p_max_reward = 10000                #Maximum reward\n",
    "p_min_reward = -100                 #Minimum reward\n",
    "\n",
    "#Rewards listed here are for documentation purpose only - change in ClipReward\n",
    "reward_goal = 10000                 #Reward for reaching the goal\n",
    "reward_forwards = 50                #Reward for moving to a new Position (\"new\" Resets on achieving a target run)\n",
    "reward_backwards = 0                #Reward for moving backwards\n",
    "reward_nooperation = 0              #Reward for standing still\n",
    "reward_crash = -100                 #Reward for colliding with a car\n",
    "\n",
    "#Type of games\n",
    "training = 1                        #0: load Model and render animation, >0: Model training without preloaded Model, 1 or < 0: Save model\n",
    "human = False                       #True: Render Mode Human shows game animation, False: No animation\n",
    "\n",
    "\n",
    "name = \"TestCroppedRAMv1_3_ClipReward\"                      #Name of to be saved/loaded model\n",
    "loadedSave=0                                                #if trained model is loaded add this to name save to avoid overwriting saved models\n",
    "path = \"C:\\\\Users\\Simon\\projektSeminar\\Modelle\"             #directory of the model\n",
    "\n",
    "#Creating the environment and the agent\n",
    "if(training<=0):\n",
    "    if(training==0):                                        #Testing loaded model without additional training\n",
    "        p_eps_end=0.0\n",
    "        p_eps_dec=0.0 \n",
    "    if(human):\n",
    "        env = ClipReward(gym.make(\"ALE/Freeway-v5\", difficulty = 1, mode = 3, obs_type = \"ram\",render_mode='human'), p_min_reward, p_max_reward)\n",
    "    else:\n",
    "        env = ClipReward(gym.make(\"ALE/Freeway-v5\", difficulty = 1, mode = 3, obs_type = \"ram\"), p_min_reward, p_max_reward)\n",
    "    agent = Agent(p_gamma, p_lr, p_epsilon, p_eps_dec, p_eps_end, p_dims_input, p_dims_fc1, p_dims_fc2, p_batch_size, p_max_mem_size, p_n_actions)\n",
    "    agent.setModel(f\"{path}\\{name}.pth\")\n",
    "else:\n",
    "    env = ClipReward(gym.make(\"ALE/Freeway-v5\", difficulty = 1, mode = 3, obs_type = \"ram\"), p_min_reward, p_max_reward)\n",
    "    agent = Agent(p_gamma, p_lr, p_epsilon, p_eps_dec, p_eps_end, p_dims_input, p_dims_fc1, p_dims_fc2, p_batch_size, p_max_mem_size, p_n_actions)\n",
    "\n",
    "#Creating lists for the cumulative reward, the epsilon history and the target runs\n",
    "cum_reward_list, eps_history_list, target_runs_list, crashes = [], [], [], []\n",
    "if(training==0):\n",
    "    for i in range(p_n_games):\n",
    "        cum_reward = 0\n",
    "        done = False\n",
    "        #Recieving the first state\n",
    "        observation = env.reset()[0]\n",
    "\n",
    "        #overriding the first state to conform input dimensions\n",
    "        ram = env.unwrapped.ale.getRAM()\n",
    "        observation=[ram[14],ram[103],ram[108], ram[109], ram[110], ram[111], ram[112], ram[113], ram[114], ram[115], ram[116], ram[117], ram[118]] \n",
    "\n",
    "        while not done:\n",
    "            #Render image\n",
    "            env.render()\n",
    "\n",
    "            #Choose an action (after conversion from int to float values)\n",
    "            action = agent.choose_action(np.float32(observation))\n",
    "\n",
    "            #Recieving the new state, reward and termination information (cutting off unnecessary information)\n",
    "            observation_, reward, done = env.step(action)[:3]\n",
    "\n",
    "            #Overriding observation_ with cropped RAM\n",
    "            ram = env.unwrapped.ale.getRAM()\n",
    "            observation_=[ram[14],ram[103],ram[108], ram[109], ram[110], ram[111], ram[112], ram[113], ram[114], ram[115], ram[116], ram[117], ram[118]] \n",
    "\n",
    "            #Updating the cumulative reward\n",
    "            cum_reward += reward\n",
    "\n",
    "            #Storing the transition\n",
    "            agent.store_transition(observation, action, reward, observation_, done)\n",
    "\n",
    "            #Getting the agent to learn\n",
    "            agent.learn()\n",
    "\n",
    "            #Updating the previous state\n",
    "            observation = observation_\n",
    "\n",
    "        #Recieving the number of target runs during the last game\n",
    "        target_runs = env.getTargetRuns()\n",
    "\n",
    "        #Appending the cumulative reward, the epsilon history and the target runs\n",
    "        cum_reward_list.append(cum_reward)\n",
    "        eps_history_list.append(agent.epsilon)\n",
    "        target_runs_list.append(target_runs)\n",
    "        crashes.append(env.getCrashCount())\n",
    "\n",
    "        #Printing the most important information\n",
    "        print('Episode: ', (i+loadedSave+1), ', Cumulative reward: ', cum_reward, ', Epsilon: %.8f' % agent.epsilon, ', Target runs: ', target_runs,'Crashes: ', crashes[i])\n",
    "\n",
    "        #Resetting the number of target runs\n",
    "        env.targetReset()\n",
    "\n",
    "        #Updating epsilon\n",
    "        Agent.decreaseEpsilon(agent)\n",
    "\n",
    "        #Saving models every p_n_games_save games (in case learning needs to be interrupted)\n",
    "        if(i % p_n_games_save == 0):\n",
    "            saving_path = f\"{path}\\{name}_{i+loadedSave}.pth\"\n",
    "            T.save(Agent.getModel(agent), saving_path)\n",
    "            if(p_epsilon_reset):\n",
    "                Agent.resetEps(agent)\n",
    "        env.close()\n",
    "else:\n",
    "    for i in range(p_n_games):\n",
    "        cum_reward = 0\n",
    "        done = False\n",
    "        #Recieving the first state\n",
    "        observation = env.reset()[0]\n",
    "\n",
    "        #overriding the first state to conform input dimensions\n",
    "        ram = env.unwrapped.ale.getRAM()\n",
    "        observation=[ram[14],ram[103],ram[108], ram[109], ram[110], ram[111], ram[112], ram[113], ram[114], ram[115], ram[116], ram[117], ram[118]] \n",
    "\n",
    "        while not done:\n",
    "            #Choose an action (after conversion from int to float values)\n",
    "            action = agent.choose_action(np.float32(observation))\n",
    "\n",
    "            #Recieving the new state, reward and termination information (cutting off unnecessary information)\n",
    "            observation_, reward, done = env.step(action)[:3]\n",
    "\n",
    "            #Overriding observation_ with cropped RAM\n",
    "            ram = env.unwrapped.ale.getRAM()\n",
    "            observation_=[ram[14],ram[103],ram[108], ram[109], ram[110], ram[111], ram[112], ram[113], ram[114], ram[115], ram[116], ram[117], ram[118]] \n",
    "\n",
    "            #Updating the cumulative reward\n",
    "            cum_reward += reward\n",
    "\n",
    "            #Storing the transition\n",
    "            agent.store_transition(observation, action, reward, observation_, done)\n",
    "\n",
    "            #Getting the agent to learn\n",
    "            agent.learn()\n",
    "\n",
    "            #Updating the previous state\n",
    "            observation = observation_\n",
    "\n",
    "        #Recieving the number of target runs during the last game\n",
    "        target_runs = env.getTargetRuns()\n",
    "\n",
    "        #Appending the cumulative reward, the epsilon history and the target runs\n",
    "        cum_reward_list.append(cum_reward)\n",
    "        eps_history_list.append(agent.epsilon)\n",
    "        target_runs_list.append(target_runs)\n",
    "        crashes.append(env.getCrashCount())\n",
    "\n",
    "        #Printing the most important information\n",
    "        print('Episode: ', (i+loadedSave+1), ', Cumulative reward: ', cum_reward, ', Epsilon: %.8f' % agent.epsilon, ', Target runs: ', target_runs,'Crashes: ', crashes[i])\n",
    "\n",
    "        #Resetting the number of target runs\n",
    "        env.targetReset()\n",
    "\n",
    "        #Updating epsilon\n",
    "        Agent.decreaseEpsilon(agent)\n",
    "\n",
    "        #Saving models every p_n_games_save games (in case learning needs to be interrupted)\n",
    "        if(i % p_n_games_save == 0):\n",
    "            saving_path = f\"{path}\\{name}_{i+loadedSave}.pth\"\n",
    "            T.save(Agent.getModel(agent), saving_path)\n",
    "            if(p_epsilon_reset):\n",
    "                Agent.resetEps(agent)\n",
    "\n",
    "\n",
    "#Saving the trained model\n",
    "if(training<0 or training==1):\n",
    "    T.save(Agent.getModel(agent), f\"{path}\\{name}.pth\")\n",
    "\n",
    "#Results and plotting:\n",
    "print('----------------------------------------------------------------------------')\n",
    "print(f\"\"\"In this run we used the follwing parameters:\n",
    "    Epsilon start value: {p_epsilon}\n",
    "    Epsilon descent value: {p_eps_dec}\n",
    "    Epsilon final value: {p_eps_end}\n",
    "    Learning rate: {p_lr}\n",
    "    Discount factor: {p_gamma}\n",
    "    Dimension of the input: {p_dims_input}\n",
    "    Dimension of the first fully connected layer: {p_dims_fc1}\n",
    "    Dimension of the second fully connected layer: {p_dims_fc2}\n",
    "    Batch size: {p_batch_size}\n",
    "    Maximum memory size: {p_max_mem_size}\n",
    "    Number of possible actions: {p_n_actions}\n",
    "    Number of games: {p_n_games}\n",
    "    Maximum reward: {p_max_reward}\n",
    "    Minimum reward: {p_min_reward}\"\"\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisierung der Ergebnisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, i)\n",
    "plt.plot(x, cum_reward_list, label = \"cumulative reward per game\")                          #Cumulative reward per game\n",
    "plt.title(f\"Cumulative reward per game ({p_n_games} games, eps. dec. = {p_eps_dec}, lr = {p_lr})\")\n",
    "plt.xlabel(\"Game\")\n",
    "plt.ylabel(\"Cumulative reward per game\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x, target_runs_list, label = \"Target runs per game\")                               #Amount of target runs\n",
    "plt.title(f\"Target runs per game ({p_n_games} games, eps. dec. = {p_eps_dec}, lr = {p_lr})\")\n",
    "plt.xlabel(\"Game\")\n",
    "plt.ylabel(\"Target runs per game\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x, eps_history_list, label = \"Epsilon value per game\")                             #Epsilon history\n",
    "plt.title(f\"Epsilon values ({p_n_games} games, eps. dec. = {p_eps_dec}, lr = {p_lr})\")\n",
    "plt.xlabel(\"Game\")\n",
    "plt.ylabel(\"Epsilon value\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x, crashes, label = \"Crashes per game\")                                            #Crashes per game\n",
    "plt.title(f\"Crashes ({p_n_games} games, eps. dec. = {p_eps_dec}, lr = {p_lr})\")\n",
    "plt.xlabel(\"Game\")\n",
    "plt.ylabel(\"Crashes\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4564dc58dfc4f7da65a8596492a0cea475f3de274280957854699bb731551874"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
