{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[atari] in c:\\users\\simon\\anaconda3\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\simon\\anaconda3\\lib\\site-packages (from gym[atari]) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\simon\\anaconda3\\lib\\site-packages (from gym[atari]) (4.11.3)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\simon\\anaconda3\\lib\\site-packages (from gym[atari]) (1.21.5)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\simon\\anaconda3\\lib\\site-packages (from gym[atari]) (0.0.8)\n",
      "Requirement already satisfied: ale-py~=0.8.0 in c:\\users\\simon\\anaconda3\\lib\\site-packages (from gym[atari]) (0.8.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\simon\\anaconda3\\lib\\site-packages (from ale-py~=0.8.0->gym[atari]) (5.10.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\simon\\anaconda3\\lib\\site-packages (from ale-py~=0.8.0->gym[atari]) (4.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\simon\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.8.0->gym[atari]) (3.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ale_py in c:\\users\\simon\\anaconda3\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\simon\\anaconda3\\lib\\site-packages (from ale_py) (4.3.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\simon\\anaconda3\\lib\\site-packages (from ale_py) (5.10.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\simon\\anaconda3\\lib\\site-packages (from ale_py) (1.21.5)\n",
      "Requirement already satisfied: importlib-metadata>=4.10.0 in c:\\users\\simon\\anaconda3\\lib\\site-packages (from ale_py) (4.11.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\simon\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.10.0->ale_py) (3.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: autorom[accept-rom-license] in c:\\users\\simon\\anaconda3\\lib\\site-packages (0.5.0)\n",
      "Requirement already satisfied: click in c:\\users\\simon\\anaconda3\\lib\\site-packages (from autorom[accept-rom-license]) (8.0.4)\n",
      "Requirement already satisfied: requests in c:\\users\\simon\\anaconda3\\lib\\site-packages (from autorom[accept-rom-license]) (2.28.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\simon\\anaconda3\\lib\\site-packages (from autorom[accept-rom-license]) (4.64.1)\n",
      "Requirement already satisfied: libtorrent in c:\\users\\simon\\anaconda3\\lib\\site-packages (from autorom[accept-rom-license]) (2.0.7)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in c:\\users\\simon\\anaconda3\\lib\\site-packages (from autorom[accept-rom-license]) (0.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\simon\\anaconda3\\lib\\site-packages (from click->autorom[accept-rom-license]) (0.4.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\simon\\anaconda3\\lib\\site-packages (from requests->autorom[accept-rom-license]) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\simon\\anaconda3\\lib\\site-packages (from requests->autorom[accept-rom-license]) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\simon\\anaconda3\\lib\\site-packages (from requests->autorom[accept-rom-license]) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\simon\\anaconda3\\lib\\site-packages (from requests->autorom[accept-rom-license]) (1.26.11)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in c:\\users\\simon\\anaconda3\\lib\\site-packages (1.13.0)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\simon\\anaconda3\\lib\\site-packages (from torch) (4.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: gym[box2d] in c:\\users\\simon\\anaconda3\\lib\\site-packages (0.26.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\simon\\anaconda3\\lib\\site-packages (from gym[box2d]) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\simon\\anaconda3\\lib\\site-packages (from gym[box2d]) (4.11.3)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\simon\\anaconda3\\lib\\site-packages (from gym[box2d]) (1.21.5)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\simon\\anaconda3\\lib\\site-packages (from gym[box2d]) (0.0.8)\n",
      "Requirement already satisfied: swig==4.* in c:\\users\\simon\\anaconda3\\lib\\site-packages (from gym[box2d]) (4.1.1)\n",
      "Collecting pygame==2.1.0\n",
      "  Using cached pygame-2.1.0-cp39-cp39-win_amd64.whl (4.8 MB)\n",
      "Collecting box2d-py==2.3.5\n",
      "  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\simon\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.8.0->gym[box2d]) (3.8.0)\n",
      "Building wheels for collected packages: box2d-py\n",
      "  Building wheel for box2d-py (setup.py): started\n",
      "  Building wheel for box2d-py (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for box2d-py\n",
      "Failed to build box2d-py\n",
      "Installing collected packages: box2d-py, pygame\n",
      "  Running setup.py install for box2d-py: started\n",
      "  Running setup.py install for box2d-py: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py bdist_wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [28 lines of output]\n",
      "      Using setuptools (version 63.4.1).\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-39\n",
      "      creating build\\lib.win-amd64-cpython-39\\Box2D\n",
      "      copying library\\Box2D\\Box2D.py -> build\\lib.win-amd64-cpython-39\\Box2D\n",
      "      copying library\\Box2D\\__init__.py -> build\\lib.win-amd64-cpython-39\\Box2D\n",
      "      creating build\\lib.win-amd64-cpython-39\\Box2D\\b2\n",
      "      copying library\\Box2D\\b2\\__init__.py -> build\\lib.win-amd64-cpython-39\\Box2D\\b2\n",
      "      running build_ext\n",
      "      building 'Box2D._Box2D' extension\n",
      "      swigging Box2D\\Box2D.i to Box2D\\Box2D_wrap.cpp\n",
      "      swig.exe -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\\Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\\Box2D_wrap.cpp Box2D\\Box2D.i\n",
      "      Box2D\\Common\\b2Math.h(67) : Warning 302: Identifier 'b2Vec2' redefined by %extend (ignored),\n",
      "      Box2D\\Box2D_math.i(47) : Warning 302: %extend definition of 'b2Vec2'.\n",
      "      Box2D\\Common\\b2Math.h(158) : Warning 302: Identifier 'b2Vec3' redefined by %extend (ignored),\n",
      "      Box2D\\Box2D_math.i(168) : Warning 302: %extend definition of 'b2Vec3'.\n",
      "      Box2D\\Common\\b2Math.h(197) : Warning 302: Identifier 'b2Mat22' redefined by %extend (ignored),\n",
      "      Box2D\\Box2D_math.i(301) : Warning 302: %extend definition of 'b2Mat22'.\n",
      "      Box2D\\Common\\b2Math.h(271) : Warning 302: Identifier 'b2Mat33' redefined by %extend (ignored),\n",
      "      Box2D\\Box2D_math.i(372) : Warning 302: %extend definition of 'b2Mat33'.\n",
      "      Box2D\\Collision\\b2DynamicTree.h(44) : Warning 312: Nested union not currently supported (ignored).\n",
      "      Box2D\\Common\\b2Settings.h(144) : Warning 506: Can't wrap varargs with keyword arguments enabled\n",
      "      Box2D\\Common\\b2Math.h(91) : Warning 509: Overloaded method b2Vec2::operator ()(int32) effectively ignored,\n",
      "      Box2D\\Common\\b2Math.h(85) : Warning 509: as it is shadowed by b2Vec2::operator ()(int32) const.\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for box2d-py\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Running setup.py install for box2d-py did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [30 lines of output]\n",
      "      Using setuptools (version 63.4.1).\n",
      "      running install\n",
      "      c:\\Users\\Simon\\anaconda3\\lib\\site-packages\\setuptools\\command\\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "        warnings.warn(\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-39\n",
      "      creating build\\lib.win-amd64-cpython-39\\Box2D\n",
      "      copying library\\Box2D\\Box2D.py -> build\\lib.win-amd64-cpython-39\\Box2D\n",
      "      copying library\\Box2D\\__init__.py -> build\\lib.win-amd64-cpython-39\\Box2D\n",
      "      creating build\\lib.win-amd64-cpython-39\\Box2D\\b2\n",
      "      copying library\\Box2D\\b2\\__init__.py -> build\\lib.win-amd64-cpython-39\\Box2D\\b2\n",
      "      running build_ext\n",
      "      building 'Box2D._Box2D' extension\n",
      "      swigging Box2D\\Box2D.i to Box2D\\Box2D_wrap.cpp\n",
      "      swig.exe -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\\Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\\Box2D_wrap.cpp Box2D\\Box2D.i\n",
      "      Box2D\\Common\\b2Math.h(67) : Warning 302: Identifier 'b2Vec2' redefined by %extend (ignored),\n",
      "      Box2D\\Box2D_math.i(47) : Warning 302: %extend definition of 'b2Vec2'.\n",
      "      Box2D\\Common\\b2Math.h(158) : Warning 302: Identifier 'b2Vec3' redefined by %extend (ignored),\n",
      "      Box2D\\Box2D_math.i(168) : Warning 302: %extend definition of 'b2Vec3'.\n",
      "      Box2D\\Common\\b2Math.h(197) : Warning 302: Identifier 'b2Mat22' redefined by %extend (ignored),\n",
      "      Box2D\\Box2D_math.i(301) : Warning 302: %extend definition of 'b2Mat22'.\n",
      "      Box2D\\Common\\b2Math.h(271) : Warning 302: Identifier 'b2Mat33' redefined by %extend (ignored),\n",
      "      Box2D\\Box2D_math.i(372) : Warning 302: %extend definition of 'b2Mat33'.\n",
      "      Box2D\\Collision\\b2DynamicTree.h(44) : Warning 312: Nested union not currently supported (ignored).\n",
      "      Box2D\\Common\\b2Settings.h(144) : Warning 506: Can't wrap varargs with keyword arguments enabled\n",
      "      Box2D\\Common\\b2Math.h(91) : Warning 509: Overloaded method b2Vec2::operator ()(int32) effectively ignored,\n",
      "      Box2D\\Common\\b2Math.h(85) : Warning 509: as it is shadowed by b2Vec2::operator ()(int32) const.\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: legacy-install-failure\n",
      "\n",
      "× Encountered error while trying to install package.\n",
      "╰─> box2d-py\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for output from the failure.\n"
     ]
    }
   ],
   "source": [
    "%pip install gym[atari]\n",
    "%pip install ale_py\n",
    "%pip install autorom[accept-rom-license]\n",
    "%pip install torch\n",
    "%pip install gym[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearReward(gym.RewardWrapper):\n",
    "    def __init__(self, env, min_reward, max_reward):\n",
    "        super().__init__(env)\n",
    "        self.pre_pos = 6\n",
    "        self.min_reward = min_reward\n",
    "        self.max_reward = max_reward\n",
    "        self.reward_range = (min_reward, max_reward)\n",
    "        self.zieldurchlaeufe = 0\n",
    "        self.crashCount=0\n",
    "\n",
    "    def reward(self, reward):\n",
    "        ram = env.unwrapped.ale.getRAM()\n",
    "        current_pos = ram[14]\n",
    "        #print(current_pos)\n",
    "        #print(ram[106])\n",
    "        reward=0\n",
    "        reward-=500/current_pos\n",
    "        reward+=current_pos\n",
    "        if(ram[106]>110):\n",
    "            reward+=10000\n",
    "        self.pre_pos = current_pos\n",
    "        if(ram[106]>0 and self.pre_pos<170):\n",
    "            #print(\"Crash!: \",ram)\n",
    "            #reward-=5\n",
    "            self.crashCount+=1\n",
    "        return np.clip(reward, self.min_reward, self.max_reward)\n",
    "\n",
    "    def getZieldurchlaeufe(self):\n",
    "        return self.zieldurchlaeufe\n",
    "    def getCrashCount(self):\n",
    "        return self.crashCount\n",
    "    def targetReset(self):\n",
    "        self.crashCount= 0\n",
    "        self.zieldurchlaeufe = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, lr, input_dims, fc1_dims, fc2_dims, n_actions):\n",
    "        #Vererbung: Aufruf des Konstruktors der Superklasse\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        #Speichern der übergebenen Parameter\n",
    "        self.input_dims = input_dims #Dimension des Inputs\n",
    "        self.fc1_dims = fc1_dims #Dimension des ersten fully connected Layer\n",
    "        self.fc2_dims = fc2_dims #Dimension des zweiten fully connected Layer\n",
    "        self.n_actions = n_actions #Anzahl der möglichen Aktionen\n",
    "        self.af=nn.LeakyReLU(0.1)#Leaky ReLU Activation Function (bc negative Reward)\n",
    "\n",
    "        #Lineare Transformation\n",
    "        #Pro Layer wird eine lineare Transformation angewandt\n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims) #* entpackt die input_dims\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.fc3 = nn.Linear(self.fc2_dims, self.n_actions)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "        #Mean squared error loss\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "        #Choosing GPU if possible\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "\n",
    "        #TODO:\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        # #Activation function (RELU)\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # x = F.leaky_relu(self.fc1(state))\n",
    "        # x = F.leaky_relu(self.fc2(x))\n",
    "        #Only applied to the first two layers, not the output\n",
    "        actions = self.fc3(x)\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, gamma, epsilon, lr, input_dims, batch_size, n_actions, max_mem_size = 1000000, eps_end = 0.01, eps_dec = 1.5e-4):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_min = eps_end\n",
    "        self.eps_dec = eps_dec\n",
    "        self.lr = lr\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.mem_size = max_mem_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mem_counter = 0\n",
    "        \n",
    "        self.Q_eval = DeepQNetwork(self.lr, n_actions = n_actions, input_dims = input_dims, fc1_dims = 256, fc2_dims= 256)\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_dims), dtype = np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_dims), dtype = np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype = np.int32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype = np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype = np.bool_)\n",
    "    \n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        #Memory wird wieder von vorne überschrieben, wenn voll\n",
    "        index = self.mem_counter % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.reward_memory[index] = reward\n",
    "        self.action_memory[index] = action\n",
    "        self.terminal_memory[index] = done\n",
    "\n",
    "        self.mem_counter += 1\n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            #Exploitation instead of exploration\n",
    "            state = T.tensor(np.array([observation])).to(self.Q_eval.device)\n",
    "            actions = self.Q_eval.forward(state)\n",
    "            #Choosing the best actionu\n",
    "            action = T.argmax(actions).item()\n",
    "        else:\n",
    "            #Exploration instead of exploitation\n",
    "            action = np.random.choice(self.action_space,p=[.1,.8,.1])#Stehen, vor, zurueck\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        if self.mem_counter < self.batch_size:\n",
    "            #Zu Beginn ist der Speicher noch leer\n",
    "            #deshalb wird der Speicher gefüllt, bevor der Agent anfängt zu lernen\n",
    "            return\n",
    "        \n",
    "        #Gradienten werden auf null gesetzt.\n",
    "        #Andernfalls werden die Gradienten in Pytorch kummuliert\n",
    "        self.Q_eval.optimizer.zero_grad()\n",
    "\n",
    "        max_mem = min(self.mem_counter, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace = False)\n",
    "\n",
    "        batch_index = np.arange(self.batch_size, dtype = np.int32)\n",
    "\n",
    "        state_batch = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n",
    "        new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\n",
    "        reward_batch = T.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\n",
    "        terminal_batch = T.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n",
    "        action_batch = self.action_memory[batch]\n",
    "\n",
    "        q_eval = self.Q_eval.forward(state_batch)[batch_index, action_batch]\n",
    "        q_next = self.Q_eval.forward(new_state_batch)\n",
    "        q_next[terminal_batch] = 0.0\n",
    "        q_target = reward_batch + self.gamma * T.max(q_next, dim = 1)[0]\n",
    "\n",
    "        loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n",
    "        loss.backward()\n",
    "        self.Q_eval.optimizer.step()\n",
    "\n",
    "        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n",
    "\n",
    "    def getModel(self):\n",
    "        return self.Q_eval\n",
    "\n",
    "    def setModel(self, path):\n",
    "        self.Q_eval = T.load(path)\n",
    "    def resetEps(self):\n",
    "        self.epsilon=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "episode  0 score -99216.03 epsilon 0.70 crash count: 1046\n",
      "0\n",
      "episode  1 score -107170.93 epsilon 0.69 crash count: 2189\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "name=\"TestRAMReLuv1_LinearReward0_\"\n",
    "\n",
    "#env = ClipReward(gym.make(\"ALE/Freeway-v5\", difficulty = 1, mode = 3, obs_type = \"ram\",frameskip=1,repeat_action_probability=0), -1000, 10000)#warum nicht in der n_games Schleife?\n",
    "agent = Agent(gamma = 0.99, epsilon = 1.0, batch_size = 64, n_actions = 3, eps_end = 0.01, input_dims = [128], lr = 0.01)\n",
    "scores, eps_history, zieldurchlaeufe, crashes = [], [], [], []\n",
    "n_games = 200\n",
    "env = LinearReward(gym.make(\"ALE/Freeway-v5\", difficulty = 1, mode = 3, obs_type = \"ram\"), -1000, 10000)\n",
    "for i in range(n_games):\n",
    "    \n",
    "    score = 0\n",
    "    done = False\n",
    "    observation = env.reset()[0]\n",
    "    while not done:\n",
    "        action = agent.choose_action(np.float32(observation))\n",
    "        observation_=observation\n",
    "        #Info2 eingefügt\n",
    "        obs, reward, done, info, info2 = env.step(action)\n",
    "        score += reward\n",
    "        agent.store_transition(observation_, action, reward, observation, done)\n",
    "        agent.learn()\n",
    "        observation = observation_\n",
    "    scores.append(score)\n",
    "    eps_history.append(agent.epsilon)\n",
    "    zieldurchlaeufe.append(LinearReward.getZieldurchlaeufe(env))\n",
    "    crashes.append(LinearReward.getCrashCount(env))\n",
    "    print(LinearReward.getZieldurchlaeufe(env))\n",
    "    print('episode ', i, 'score %.2f' % score, 'epsilon %.2f' % agent.epsilon,\"crash count: %i\" %LinearReward.getCrashCount(env))\n",
    "    if(i%50==0): # Save model every 50 games in case learning needs to be interrupted\n",
    "        agent.resetEps()\n",
    "        s=f\"C:\\\\Users\\Simon\\projektSeminar\\DeepLearningProjekSeminar-main V1\\{name}{i}.pth\"\n",
    "        T.save(Agent.getModel(agent), s)\n",
    "x = [i + 1 for i in range(n_games)]\n",
    "\n",
    "\n",
    "T.save(Agent.getModel(agent), f\"C:\\\\Users\\Simon\\projektSeminar\\Modelle\\{name}.pth\")\n",
    "\n",
    "plt.plot(x, scores)\n",
    "plt.ylabel=\"Scores\"\n",
    "plt.show()\n",
    "plt.plot(x,zieldurchlaeufe)\n",
    "plt.ylabel=\"Zieldurchlaeufe\"\n",
    "plt.show()\n",
    "plt.plot(x,eps_history)\n",
    "plt.ylabel=\"Epsilon\"\n",
    "plt.show()\n",
    "plt.plot(x,crashes)\n",
    "plt.ylabel=\"Crashes\"\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4564dc58dfc4f7da65a8596492a0cea475f3de274280957854699bb731551874"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
